{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from tqdm import tqdm\n",
    "import tensorboard\n",
    "import tensorboardX\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from log import Logger\n",
    "from data import gDataset, trainDataset, testDataset\n",
    "from util import r2, mse, rmse, mae, pp_mse, pp_rmse, pp_mae\n",
    "from model import autoencoder_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1000], loss:0.0350, MSE_loss:0.0162\n",
      " epoch [1/1000],test_loss:0.0141, test_MSE_loss:0.0018\n",
      "epoch [2/1000], loss:0.0112, MSE_loss:0.0009\n",
      " epoch [2/1000],test_loss:0.0105, test_MSE_loss:0.0008\n",
      "epoch [3/1000], loss:0.0102, MSE_loss:0.0007\n",
      " epoch [3/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [4/1000], loss:0.0102, MSE_loss:0.0006\n",
      " epoch [4/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [5/1000], loss:0.0102, MSE_loss:0.0006\n",
      " epoch [5/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [6/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [6/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [7/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [7/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [8/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [8/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [9/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [9/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [10/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [10/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [11/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [11/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [12/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [12/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [13/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [13/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [14/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [14/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [15/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [15/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [16/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [16/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [17/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [17/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [18/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [18/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [19/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [19/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [20/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [20/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [21/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [21/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [22/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [22/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [23/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [23/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [24/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [24/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [25/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [25/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [26/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [26/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [27/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [27/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [28/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [28/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [29/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [29/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [30/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [30/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [31/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [31/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [32/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [32/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [33/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [33/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [34/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [34/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [35/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [35/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [36/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [36/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [37/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [37/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [38/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [38/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [39/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [39/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [40/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [40/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [41/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [41/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [42/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [42/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [43/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [43/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [44/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [44/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [45/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [45/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [46/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [46/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [47/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [47/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [48/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [48/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [49/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [49/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [50/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [50/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [51/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [51/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [52/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [52/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [53/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [53/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [54/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [54/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [55/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [55/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [56/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [56/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [57/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [57/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [58/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [58/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [59/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [59/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [60/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [60/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [61/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [61/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [62/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [62/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [63/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [63/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [64/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [64/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [65/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [65/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [66/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [66/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [67/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [67/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [68/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [68/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [69/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [69/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [70/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [70/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [71/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [71/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [72/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [72/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [73/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [73/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [74/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [74/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [75/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [75/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [76/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [76/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [77/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [77/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [78/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [78/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [79/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [79/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [80/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [80/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [81/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [81/1000],test_loss:0.0103, test_MSE_loss:0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [82/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [82/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [83/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [83/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [84/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [84/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [85/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [85/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [86/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [86/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [87/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [87/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [88/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [88/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [89/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [89/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [90/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [90/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [91/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [91/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [92/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [92/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [93/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [93/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [94/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [94/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [95/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [95/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [96/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [96/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [97/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [97/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [98/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [98/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [99/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [99/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [100/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [100/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [101/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [101/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [102/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [102/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [103/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [103/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [104/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [104/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [105/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [105/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [106/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [106/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [107/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [107/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [108/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [108/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [109/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [109/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [110/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [110/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [111/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [111/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [112/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [112/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [113/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [113/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [114/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [114/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [115/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [115/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [116/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [116/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [117/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [117/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [118/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [118/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [119/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [119/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [120/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [120/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [121/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [121/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [122/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [122/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [123/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [123/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [124/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [124/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [125/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [125/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [126/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [126/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [127/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [127/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [128/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [128/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [129/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [129/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [130/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [130/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [131/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [131/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [132/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [132/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [133/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [133/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [134/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [134/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [135/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [135/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [136/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [136/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [137/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [137/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [138/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [138/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [139/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [139/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [140/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [140/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [141/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [141/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [142/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [142/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [143/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [143/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [144/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [144/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [145/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [145/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [146/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [146/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [147/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [147/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [148/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [148/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [149/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [149/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [150/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [150/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [151/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [151/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [152/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [152/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [153/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [153/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [154/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [154/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [155/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [155/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [156/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [156/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [157/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [157/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [158/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [158/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [159/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [159/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [160/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [160/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [161/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [161/1000],test_loss:0.0103, test_MSE_loss:0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [162/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [162/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [163/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [163/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [164/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [164/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [165/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [165/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [166/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [166/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [167/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [167/1000],test_loss:0.0103, test_MSE_loss:0.0008\n",
      "epoch [168/1000], loss:0.0101, MSE_loss:0.0006\n",
      " epoch [168/1000],test_loss:0.0103, test_MSE_loss:0.0008\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./gal_img'):\n",
    "    os.mkdir('./gal_img')\n",
    "\n",
    "    \n",
    "    \n",
    "def to_img(x):   # image size 56 * 56 \n",
    "    x = x.view(x.size(0), 1, 56, 56)\n",
    "    return x\n",
    "\n",
    "dataset= trainDataset()\n",
    "dataloader= DataLoader(dataset=dataset, batch_size=64,shuffle=True)\n",
    "\n",
    "test_dataset = testDataset()\n",
    "test_dataloader= DataLoader(dataset=test_dataset, batch_size=64,shuffle=True)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(\"exp\",filename_suffix='plot_')\n",
    "\n",
    "\n",
    "num_epochs =1000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = autoencoder_6().cuda()\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "#scheduler \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150,250,400,600,800], gamma=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    total_mse = 0.0 \n",
    "    num_examples = 0.0\n",
    "    test_num_examples=0.0\n",
    "    \n",
    "    \n",
    "    test_total_loss = 0.0    \n",
    "    test_total_mse=0.0\n",
    "    \n",
    "    model.train()\n",
    "    for data in dataloader:\n",
    "        img = data\n",
    "        img = img.type(torch.float32)\n",
    "        img = img.view(img.size(0), 1,56,56)\n",
    "        img = img.cuda()\n",
    "       # print(img.shape)\n",
    "        \n",
    "\n",
    "        # forward\n",
    "        output = model(img)\n",
    "                \n",
    "       #print(\"output \",output.shape)\n",
    "        loss = criterion(output.cuda(), img)\n",
    "        MSE_loss = nn.MSELoss()(output.cuda(), img)\n",
    "        batch_size = img.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_mse += MSE_loss.item() * batch_size\n",
    "        num_examples += batch_size\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "    # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    model.eval()\n",
    "    for data in test_dataloader:\n",
    "        test_img = data\n",
    "        test_img = test_img.type(torch.float32)\n",
    "        test_img = test_img.view(test_img.size(0), 1,56,56)\n",
    "        test_img = test_img.cuda()\n",
    "       # print(img.shape)\n",
    "        \n",
    "\n",
    "        # forward\n",
    "        test_output = model(test_img)\n",
    "                \n",
    "       #print(\"output \",output.shape)\n",
    "        test_loss = criterion(test_output.cuda(), test_img)\n",
    "        test_MSE_loss = nn.MSELoss()(test_output.cuda(), test_img)\n",
    "        batch_size = test_img.size(0)\n",
    "        test_total_loss += test_loss.item() * batch_size\n",
    "        test_total_mse += test_MSE_loss.item() * batch_size\n",
    "        test_num_examples += batch_size\n",
    "\n",
    "        \n",
    "    writer.add_scalar('Loss/train',total_loss / num_examples,epoch)\n",
    "    writer.add_scalar('Mse/train', total_mse / num_examples,epoch)        \n",
    "    writer.add_scalar('Loss/test',test_total_loss / test_num_examples,epoch)\n",
    "    writer.add_scalar('Mse/test', test_total_mse / test_num_examples,epoch)\n",
    "    \n",
    "\n",
    "        \n",
    "    ''' \n",
    "    print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, total_loss / num_examples, total_mse/ num_examples))    \n",
    "    print(' epoch [{}/{}],test_loss:{:.4f}, test_MSE_loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, test_total_loss / test_num_examples, test_total_mse/ test_num_examples))\n",
    "    '''\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        x = to_img(img.cpu().data)\n",
    "        x_hat = to_img(output.cpu().data)\n",
    "        torch.save(x, './gal_img/sc_sche6_x_{}.pt'.format(epoch))\n",
    "        torch.save(x_hat, './gal_img/sc_sche6_x_hat_{}.pt'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './gal_img/sc_sche6_sim_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = loss.cpu().detach().numpy()\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = visdom.Visdom()\n",
    "\n",
    "\n",
    "loss_window = vis.line(\n",
    "    X=torch.zeros((1,)).cpu(),\n",
    "    Y=torch.zeros((1)).cpu(),\n",
    "    opts=dict(xlabel='epoch',ylabel='Loss',title='training loss',legend=['Loss']))\n",
    "\n",
    "vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([loss.data]).unsqueeze(0).cpu(),win=loss_window,update='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder_5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder_5, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2), \n",
    "            \n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1),  \n",
    "            \n",
    "            nn.Conv2d(8, 2, 3, stride=1, padding=1),  # b, 2,\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1),  # b, 2,\n",
    "            \n",
    "            nn.Conv2d(2, 1, 3, stride=2, padding=1),  # b, 1, 3, 3 \n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            # output = (input - 1 ) * stride -2 * padding + kernel size\n",
    "            \n",
    "            nn.ConvTranspose2d(1, 2, 3, stride=1),  # b, 2,  4 , 4 \n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(2, 8, 3, stride=3, padding =1  ),  # b, 8, 10 , 10 \n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding =1  ),  # b, 8, 19 , 19 \n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(8, 4, 3, stride=3, padding =1  ),  # b, 8,  55, 55  \n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(4, 1, 3, stride=3, padding =1  ),  # b, 8,  55, 55  \n",
    "            nn.Sigmoid()\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=23):\n",
    "        self.inplanes = 64\n",
    "        super (Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)#, return_indices = True)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, 1000)\n",
    "\t#self.fc = nn.Linear(num_classes,16) \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\t\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\t\n",
    "        x = self.maxpool(x)\n",
    "\t\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "encoder = Encoder(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "encoder.fc = nn.Linear(2048, 48)\n",
    "\n",
    "encoder=encoder.cuda()\n",
    "y=torch.rand(1,3,224,224)\n",
    "x=torch.rand(1,128)\n",
    "x=Variable(x.cuda())\n",
    "\n",
    "zsize=10  \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Decoder,self).__init__()\n",
    "\t\tself.dfc3 = nn.Linear(zsize, 4096)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(4096)\n",
    "\t\tself.dfc2 = nn.Linear(4096, 4096)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(4096)\n",
    "\t\tself.dfc1 = nn.Linear(4096,256 * 6 * 6)\n",
    "\t\tself.bn1 = nn.BatchNorm2d(256*6*6)\n",
    "\t\tself.upsample1=nn.Upsample(scale_factor=2)\n",
    "\t\tself.dconv5 = nn.ConvTranspose2d(256, 256, 3, padding = 0)\n",
    "\t\tself.dconv4 = nn.ConvTranspose2d(256, 384, 3, padding = 1)\n",
    "\t\tself.dconv3 = nn.ConvTranspose2d(384, 192, 3, padding = 1)\n",
    "\t\tself.dconv2 = nn.ConvTranspose2d(192, 64, 5, padding = 2)\n",
    "\t\tself.dconv1 = nn.ConvTranspose2d(64, 3, 12, stride = 4, padding = 4)\n",
    "\n",
    "\tdef forward(self,x):#,i1,i2,i3):\n",
    "\t\t\n",
    "\t\tx = self.dfc3(x)\n",
    "\t\t#x = F.relu(x)\n",
    "\t\tx = F.relu(self.bn3(x))\n",
    "\t\t\n",
    "\t\tx = self.dfc2(x)\n",
    "\t\tx = F.relu(self.bn2(x))\n",
    "\t\t#x = F.relu(x)\n",
    "\t\tx = self.dfc1(x)\n",
    "\t\tx = F.relu(self.bn1(x))\n",
    "\t\t#x = F.relu(x)\n",
    "\t\t#print(x.size())\n",
    "\t\tx = x.view(batch_size,256,6,6)\n",
    "\t\t#print (x.size())\n",
    "\t\tx=self.upsample1(x)\n",
    "\t\t#print x.size()\n",
    "\t\tx = self.dconv5(x)\n",
    "\t\t#print x.size()\n",
    "\t\tx = F.relu(x)\n",
    "\t\t#print x.size()\n",
    "\t\tx = F.relu(self.dconv4(x))\n",
    "\t\t#print x.size()\n",
    "\t\tx = F.relu(self.dconv3(x))\n",
    "\t\t#print x.size()\t\t\n",
    "\t\tx=self.upsample1(x)\n",
    "\t\t#print x.size()\t\t\n",
    "\t\tx = self.dconv2(x)\n",
    "\t\t#print x.size()\t\t\n",
    "\t\tx = F.relu(x)\n",
    "\t\tx=self.upsample1(x)\n",
    "\t\t#print x.size()\n",
    "\t\tx = self.dconv1(x)\n",
    "\t\t#print x.size()\n",
    "\t\tx = F.sigmoid(x)\n",
    "\t\t#print x\n",
    "\t\treturn x\n",
    "decoder = Decoder()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(autoencoder,self).__init__()\n",
    "\t\tself.encoder = encoder(x)\n",
    "\t\tself.decoder = decoder()\n",
    "\n",
    "\tdef forward(self,x):\n",
    "\t\t#x=Encoder(x)\n",
    "\t\tx = self.encoder(x)\n",
    "\n",
    "\t\tx = self.decoder(x)\n",
    "\t\treturn x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_sample_img(img, name):\n",
    "    img = img.view(1, 56, 56)\n",
    "    save_image(img, './sample_{}.png'.format(name))\n",
    "\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
