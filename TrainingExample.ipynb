{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from log import Logger\n",
    "from data import gDataset, trainDataset, testDataset\n",
    "from util import r2, mse, rmse, mae, pp_mse, pp_rmse, pp_mae\n",
    "#from model import autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./gal_img'):\n",
    "    os.mkdir('./gal_img')\n",
    "\n",
    "    \n",
    "    \n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 1, 96, 96)\n",
    "    return x\n",
    "\n",
    "num_epochs =1000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "def plot_sample_img(img, name):\n",
    "    img = img.view(1, 96, 96)\n",
    "    save_image(img, './sample_{}.png'.format(name))\n",
    "\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 28, 28\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 13, 13\n",
    "            \n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 7, 7\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1),  # b, 8, 6, 6\n",
    "            \n",
    "            nn.Conv2d(8, 2, 3, stride=1, padding=1),  # b, 2, 6, 6\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1),  # b, 2, 5, 5 \n",
    "            \n",
    "            nn.Conv2d(2, 1, 3, stride=2, padding=1),  # b, 1, 3, 3\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(1, 2, 3, stride=1, padding =1 ),  # b, 2, \n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(2, 8, 3, stride=1),  # b, 8, \n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, \n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(16, 8, 2, stride=3),  # b, 8, \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=3),  # b, 1, 96, 96\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= gDataset()\n",
    "dataloader= DataLoader(dataset=dataset, batch_size=64,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1000], loss:0.6451, MSE_loss:0.2234\n",
      "epoch [2/1000], loss:0.6275, MSE_loss:0.2140\n",
      "epoch [3/1000], loss:0.4830, MSE_loss:0.1544\n",
      "epoch [4/1000], loss:0.3817, MSE_loss:0.1158\n",
      "epoch [5/1000], loss:0.4193, MSE_loss:0.1101\n",
      "epoch [6/1000], loss:0.3696, MSE_loss:0.1041\n",
      "epoch [7/1000], loss:0.3510, MSE_loss:0.1013\n",
      "epoch [8/1000], loss:0.3555, MSE_loss:0.0973\n",
      "epoch [9/1000], loss:0.3330, MSE_loss:0.0944\n",
      "epoch [10/1000], loss:0.3102, MSE_loss:0.0893\n",
      "epoch [11/1000], loss:0.2888, MSE_loss:0.0847\n",
      "epoch [12/1000], loss:0.2847, MSE_loss:0.0792\n",
      "epoch [13/1000], loss:0.2686, MSE_loss:0.0736\n",
      "epoch [14/1000], loss:0.2629, MSE_loss:0.0695\n",
      "epoch [15/1000], loss:0.2356, MSE_loss:0.0629\n",
      "epoch [16/1000], loss:0.2248, MSE_loss:0.0574\n",
      "epoch [17/1000], loss:0.2098, MSE_loss:0.0525\n",
      "epoch [18/1000], loss:0.1970, MSE_loss:0.0477\n",
      "epoch [19/1000], loss:0.1863, MSE_loss:0.0429\n",
      "epoch [20/1000], loss:0.1756, MSE_loss:0.0386\n",
      "epoch [21/1000], loss:0.1640, MSE_loss:0.0344\n",
      "epoch [22/1000], loss:0.1569, MSE_loss:0.0304\n",
      "epoch [23/1000], loss:0.1446, MSE_loss:0.0271\n",
      "epoch [24/1000], loss:0.1345, MSE_loss:0.0240\n",
      "epoch [25/1000], loss:0.1268, MSE_loss:0.0210\n",
      "epoch [26/1000], loss:0.1182, MSE_loss:0.0186\n",
      "epoch [27/1000], loss:0.1110, MSE_loss:0.0162\n",
      "epoch [28/1000], loss:0.1023, MSE_loss:0.0142\n",
      "epoch [29/1000], loss:0.0944, MSE_loss:0.0124\n",
      "epoch [30/1000], loss:0.0950, MSE_loss:0.0115\n",
      "epoch [31/1000], loss:0.0883, MSE_loss:0.0101\n",
      "epoch [32/1000], loss:0.0755, MSE_loss:0.0082\n",
      "epoch [33/1000], loss:0.0747, MSE_loss:0.0070\n",
      "epoch [34/1000], loss:0.0688, MSE_loss:0.0063\n",
      "epoch [35/1000], loss:0.0619, MSE_loss:0.0053\n",
      "epoch [36/1000], loss:0.0581, MSE_loss:0.0046\n",
      "epoch [37/1000], loss:0.0575, MSE_loss:0.0039\n",
      "epoch [38/1000], loss:0.0537, MSE_loss:0.0034\n",
      "epoch [39/1000], loss:0.0515, MSE_loss:0.0030\n",
      "epoch [40/1000], loss:0.0489, MSE_loss:0.0028\n",
      "epoch [41/1000], loss:0.0456, MSE_loss:0.0023\n",
      "epoch [42/1000], loss:0.0425, MSE_loss:0.0020\n",
      "epoch [43/1000], loss:0.0447, MSE_loss:0.0017\n",
      "epoch [44/1000], loss:0.0368, MSE_loss:0.0015\n",
      "epoch [45/1000], loss:0.0513, MSE_loss:0.0044\n",
      "epoch [46/1000], loss:0.0393, MSE_loss:0.0012\n",
      "epoch [47/1000], loss:0.0360, MSE_loss:0.0012\n",
      "epoch [48/1000], loss:0.0276, MSE_loss:0.0009\n",
      "epoch [49/1000], loss:0.0306, MSE_loss:0.0008\n",
      "epoch [50/1000], loss:0.0279, MSE_loss:0.0007\n",
      "epoch [51/1000], loss:0.0271, MSE_loss:0.0006\n",
      "epoch [52/1000], loss:0.0265, MSE_loss:0.0006\n",
      "epoch [53/1000], loss:0.0254, MSE_loss:0.0005\n",
      "epoch [54/1000], loss:0.0253, MSE_loss:0.0005\n",
      "epoch [55/1000], loss:0.0251, MSE_loss:0.0005\n",
      "epoch [56/1000], loss:0.0223, MSE_loss:0.0004\n",
      "epoch [57/1000], loss:0.0235, MSE_loss:0.0004\n",
      "epoch [58/1000], loss:0.0221, MSE_loss:0.0004\n",
      "epoch [59/1000], loss:0.0201, MSE_loss:0.0003\n",
      "epoch [60/1000], loss:0.0292, MSE_loss:0.0022\n",
      "epoch [61/1000], loss:0.0207, MSE_loss:0.0003\n",
      "epoch [62/1000], loss:0.0190, MSE_loss:0.0003\n",
      "epoch [63/1000], loss:0.0203, MSE_loss:0.0003\n",
      "epoch [64/1000], loss:0.0350, MSE_loss:0.0029\n",
      "epoch [65/1000], loss:0.0208, MSE_loss:0.0003\n",
      "epoch [66/1000], loss:0.0169, MSE_loss:0.0002\n",
      "epoch [67/1000], loss:0.0194, MSE_loss:0.0003\n",
      "epoch [68/1000], loss:0.0219, MSE_loss:0.0003\n",
      "epoch [69/1000], loss:0.0201, MSE_loss:0.0007\n",
      "epoch [70/1000], loss:0.0164, MSE_loss:0.0002\n",
      "epoch [71/1000], loss:0.0231, MSE_loss:0.0003\n",
      "epoch [72/1000], loss:0.0148, MSE_loss:0.0002\n",
      "epoch [73/1000], loss:0.0211, MSE_loss:0.0002\n",
      "epoch [74/1000], loss:0.0189, MSE_loss:0.0003\n",
      "epoch [75/1000], loss:0.0262, MSE_loss:0.0007\n",
      "epoch [76/1000], loss:0.0211, MSE_loss:0.0003\n",
      "epoch [77/1000], loss:0.0153, MSE_loss:0.0002\n",
      "epoch [78/1000], loss:0.0182, MSE_loss:0.0003\n",
      "epoch [79/1000], loss:0.0233, MSE_loss:0.0006\n",
      "epoch [80/1000], loss:0.0209, MSE_loss:0.0003\n",
      "epoch [81/1000], loss:0.0174, MSE_loss:0.0002\n",
      "epoch [82/1000], loss:0.0277, MSE_loss:0.0007\n",
      "epoch [83/1000], loss:0.0326, MSE_loss:0.0030\n",
      "epoch [84/1000], loss:0.0210, MSE_loss:0.0003\n",
      "epoch [85/1000], loss:0.0205, MSE_loss:0.0008\n",
      "epoch [86/1000], loss:0.0164, MSE_loss:0.0002\n",
      "epoch [87/1000], loss:0.0208, MSE_loss:0.0003\n",
      "epoch [88/1000], loss:0.0261, MSE_loss:0.0016\n",
      "epoch [89/1000], loss:0.0192, MSE_loss:0.0003\n",
      "epoch [90/1000], loss:0.0174, MSE_loss:0.0002\n",
      "epoch [91/1000], loss:0.0208, MSE_loss:0.0003\n",
      "epoch [92/1000], loss:0.0222, MSE_loss:0.0004\n",
      "epoch [93/1000], loss:0.0160, MSE_loss:0.0002\n",
      "epoch [94/1000], loss:0.0265, MSE_loss:0.0004\n",
      "epoch [95/1000], loss:0.0266, MSE_loss:0.0004\n",
      "epoch [96/1000], loss:0.0240, MSE_loss:0.0006\n",
      "epoch [97/1000], loss:0.0258, MSE_loss:0.0006\n",
      "epoch [98/1000], loss:0.0208, MSE_loss:0.0003\n",
      "epoch [99/1000], loss:0.0187, MSE_loss:0.0008\n",
      "epoch [100/1000], loss:0.0163, MSE_loss:0.0002\n",
      "epoch [101/1000], loss:0.0177, MSE_loss:0.0003\n",
      "epoch [102/1000], loss:0.0216, MSE_loss:0.0003\n",
      "epoch [103/1000], loss:0.0177, MSE_loss:0.0002\n",
      "epoch [104/1000], loss:0.0272, MSE_loss:0.0006\n",
      "epoch [105/1000], loss:0.0367, MSE_loss:0.0028\n",
      "epoch [106/1000], loss:0.0184, MSE_loss:0.0003\n",
      "epoch [107/1000], loss:0.0283, MSE_loss:0.0017\n",
      "epoch [108/1000], loss:0.0255, MSE_loss:0.0004\n",
      "epoch [109/1000], loss:0.0235, MSE_loss:0.0014\n",
      "epoch [110/1000], loss:0.0215, MSE_loss:0.0003\n",
      "epoch [111/1000], loss:0.0152, MSE_loss:0.0002\n",
      "epoch [112/1000], loss:0.0160, MSE_loss:0.0002\n",
      "epoch [113/1000], loss:0.0235, MSE_loss:0.0003\n",
      "epoch [114/1000], loss:0.0245, MSE_loss:0.0003\n",
      "epoch [115/1000], loss:0.0291, MSE_loss:0.0013\n",
      "epoch [116/1000], loss:0.0239, MSE_loss:0.0003\n",
      "epoch [117/1000], loss:0.0197, MSE_loss:0.0002\n",
      "epoch [118/1000], loss:0.0200, MSE_loss:0.0002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2cbf676d3a9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img = data\n",
    "        \n",
    "        \n",
    "        \n",
    "        img.type(torch.float32)\n",
    "\n",
    "        img = img.view(img.size(0), 1,96,96)\n",
    "        img = Variable(img).cuda().type('torch.FloatTensor')\n",
    "        \n",
    "       # print(img.shape)\n",
    "        \n",
    "\n",
    "        # forward\n",
    "\n",
    "        output = model(img.cuda())\n",
    "        \n",
    "   #     print(\"output \",output.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        loss = criterion(output.cuda(), img.cuda())\n",
    "        \n",
    "        MSE_loss = nn.MSELoss()(output.cuda(), img.cuda())\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # log\n",
    "    print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data, MSE_loss.data))\n",
    "    if epoch % 10 == 0:\n",
    "        x = to_img(img.cpu().data)\n",
    "        x_hat = to_img(output.cpu().data)\n",
    "        save_image(x, './gal_img/x_{}.png'.format(epoch))\n",
    "        save_image(x_hat, './gal_img/x_hat_{}.png'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './sim_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "96*96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=23):\n",
    "        self.inplanes = 64\n",
    "        super (Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)#, return_indices = True)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, 1000)\n",
    "\t#self.fc = nn.Linear(num_classes,16) \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\t\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\t\n",
    "        x = self.maxpool(x)\n",
    "\t\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "encoder = Encoder(Bottleneck, [3, 4, 6, 3])\n",
    "#loaded_weights = torch.load('/home/siplab/Saket/resnet18-5c106cde.pth')\n",
    "#print encoder.layer1[1].conv1.weight.data[0][0]\n",
    "encoder.fc = nn.Linear(2048, 48)\n",
    "#for param in encoder.parameters():\n",
    "#    param.requires_grad = False\n",
    "encoder=encoder.cuda()\n",
    "y=torch.rand(1,3,224,224)\n",
    "x=torch.rand(1,128)\n",
    "x=Variable(x.cuda())\n",
    "#print decoder(x)\n",
    "#y=Variable(y.cuda())\n",
    "#print(\"\\n\")\n",
    "#encoder(y)\n",
    "#print encoder(y)    \n",
    "    \n",
    "zsize=100   \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Decoder,self).__init__()\n",
    "\t\tself.dfc3 = nn.Linear(zsize, 4096)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(4096)\n",
    "\t\tself.dfc2 = nn.Linear(4096, 4096)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(4096)\n",
    "\t\tself.dfc1 = nn.Linear(4096,256 * 6 * 6)\n",
    "\t\tself.bn1 = nn.BatchNorm2d(256*6*6)\n",
    "\t\tself.upsample1=nn.Upsample(scale_factor=2)\n",
    "\t\tself.dconv5 = nn.ConvTranspose2d(256, 256, 3, padding = 0)\n",
    "\t\tself.dconv4 = nn.ConvTranspose2d(256, 384, 3, padding = 1)\n",
    "\t\tself.dconv3 = nn.ConvTranspose2d(384, 192, 3, padding = 1)\n",
    "\t\tself.dconv2 = nn.ConvTranspose2d(192, 64, 5, padding = 2)\n",
    "\t\tself.dconv1 = nn.ConvTranspose2d(64, 3, 12, stride = 4, padding = 4)\n",
    "\n",
    "\tdef forward(self,x):#,i1,i2,i3):\n",
    "\t\t\n",
    "\t\tx = self.dfc3(x)\n",
    "\t\t#x = F.relu(x)\n",
    "\t\tx = F.relu(self.bn3(x))\n",
    "\t\t\n",
    "\t\tx = self.dfc2(x)\n",
    "\t\tx = F.relu(self.bn2(x))\n",
    "\t\t#x = F.relu(x)\n",
    "\t\tx = self.dfc1(x)\n",
    "\t\tx = F.relu(self.bn1(x))\n",
    "\t\t#x = F.relu(x)\n",
    "\t\t#print(x.size())\n",
    "\t\tx = x.view(batch_size,256,6,6)\n",
    "\t\t#print (x.size())\n",
    "\t\tx=self.upsample1(x)\n",
    "\t\t#print x.size()\n",
    "\t\tx = self.dconv5(x)\n",
    "\t\t#print x.size()\n",
    "\t\tx = F.relu(x)\n",
    "\t\t#print x.size()\n",
    "\t\tx = F.relu(self.dconv4(x))\n",
    "\t\t#print x.size()\n",
    "\t\tx = F.relu(self.dconv3(x))\n",
    "\t\t#print x.size()\t\t\n",
    "\t\tx=self.upsample1(x)\n",
    "\t\t#print x.size()\t\t\n",
    "\t\tx = self.dconv2(x)\n",
    "\t\t#print x.size()\t\t\n",
    "\t\tx = F.relu(x)\n",
    "\t\tx=self.upsample1(x)\n",
    "\t\t#print x.size()\n",
    "\t\tx = self.dconv1(x)\n",
    "\t\t#print x.size()\n",
    "\t\tx = F.sigmoid(x)\n",
    "\t\t#print x\n",
    "\t\treturn x\n",
    "decoder = Decoder()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(autoencoder,self).__init__()\n",
    "\t\tself.encoder = encoder(x)\n",
    "\t\tself.decoder = decoder()\n",
    "\n",
    "\tdef forward(self,x):\n",
    "\t\t#x=Encoder(x)\n",
    "\t\tx = self.encoder(x)\n",
    "\n",
    "\t\tx = self.decoder(x)\n",
    "\t\treturn x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
